# -*- coding: utf-8 -*-
"""WAF_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-vm6TX__A3ijpFu4aSxaaYrfBI1hOOAL

# **Loading the Libraries**
"""

import pandas as pd
import joblib

# ML models
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# Preprocessing
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# Metrics
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    classification_report,
    confusion_matrix,
    roc_auc_score,
    roc_curve
)

# Plots
import matplotlib.pyplot as plt
import seaborn as sns

"""# **Data Preproccesing**"""

df = pd.read_csv("/content/csic_database.csv", low_memory=False)

print("Dataset loaded:", df.shape)

print(df.columns.tolist())

print(df.head())

if "URL" in df.columns and "content" in df.columns:
    df["request_text"] = df["URL"].astype(str) + " " + df["content"].astype(str)
    text_col = "request_text"
    print("Using merged text column:", text_col)
else:
    # fallback to your heuristic
    possible_text = ['content','path','uri','url','request','payload','body','params','query']
    text_col = next((c for c in possible_text if c in df.columns), None)

    if text_col is None:
        obj_cols = [c for c in df.columns if df[c].dtype == 'object']
        text_col = max(obj_cols, key=lambda c: df[c].astype(str).map(len).mean())

    print("Using text column:", text_col)

possible_label_cols = ['label', 'classification', 'class', 'target', 'type']

for col in possible_label_cols:
    if col in df.columns:
        label_col = col
        break
else:
    raise SystemExit("No valid label column found. Please set label_col manually.")

print("Using label column:", label_col)
print(df[label_col].value_counts())

"""**Data Cleaning**"""

columns_to_drop = [
    "Unnamed: 0",
    "User-Agent",
    "Pragma",
    "Cache-Control",
    "Accept",
    "Accept-encoding",
    "Accept-charset",
    "language",
    "host",
    "cookie",
    "content-type",
    "connection",
    "lenght"
]

df = df.drop(columns=columns_to_drop, errors="ignore")

print("New columns:", df.columns.tolist())

df.to_csv("csic_database_cleaned.csv", index=False)

df.head()

df.shape

"""# **Feature Engineering**"""

print(df['content'].isna().sum())

df["request_text"] = df["URL"].astype(str) + " " + df["content"].astype(str)

df

df = df.rename(columns={"classification": "label"})

print(df["label"].value_counts())

X = df["request_text"]
y = df["label"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

vectorizer = TfidfVectorizer(
    max_features=5000,
    ngram_range=(1,2),
    analyzer="char",
)

X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

"""# **Model Training & Performance Metrics**"""

models = {
    "Logistic Regression": LogisticRegression(max_iter=500),
    "SVM (LinearSVC)": LinearSVC(),
    "Random Forest": RandomForestClassifier(n_estimators=200),
    "XGBoost": XGBClassifier(
        n_estimators=300,
        learning_rate=0.1,
        max_depth=6,
        eval_metric="logloss",
        use_label_encoder=False
    ),
    "LightGBM": LGBMClassifier(
        n_estimators=300,
        learning_rate=0.1
    )
}

results = {}

for name, model in models.items():
    print(f"\n===============================\nTraining: {name}\n===============================")

    model.fit(X_train_vec, y_train)
    preds = model.predict(X_test_vec)

    acc = accuracy_score(y_test, preds)
    f1 = f1_score(y_test, preds, average="weighted")

    print(f"Accuracy: {acc:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(classification_report(y_test, preds))

    results[name] = acc

print("\n\n===== MODEL PERFORMANCE SUMMARY =====")
for m, a in results.items():
    print(f"{m:25} → {a:.4f}")

best_model = max(results, key=results.get)
print(f"\n Best Model = {best_model}")

"""# **Curves & Plotting**"""

cm = confusion_matrix(y_test, preds)

plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title(f"Confusion Matrix - {name}")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Save TF-IDF vectorizer
joblib.dump(vectorizer, "tfidf_vectorizer.pkl")

# Save best model
joblib.dump(models[best_model], "waf_best_model.pkl")

print("Model and vectorizer saved successfully!")

plt.figure(figsize=(8, 6))

# Generate ROC curves
for name, model in models.items():
    print(f"Training {name}...")

    model.fit(X_train_vec, y_train)

    # Probability scores
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test_vec)[:, 1]
    else:
        y_proba = model.decision_function(X_test_vec)

    # ROC-AUC Score
    auc = roc_auc_score(y_test, y_proba)
    print(f"{name} AUC: {auc:.4f}")

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    plt.plot(fpr, tpr, label=f"{name} (AUC={auc:.3f})")

# Plot formatting
plt.plot([0, 1], [0, 1], "k--")  # diagonal line
plt.title("ROC–AUC Curve Comparison")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.grid(True)
plt.show()

y_proba = model.predict_proba(X_test_vec)[:, 1]

# AUC Score
auc = roc_auc_score(y_test, y_proba)
print(f"LightGBM ROC-AUC: {auc:.4f}")

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba)

# Plot ROC
plt.figure(figsize=(7, 5))
plt.plot(fpr, tpr, label=f"LightGBM (AUC={auc:.3f})")
plt.plot([0, 1], [0, 1], "k--")  # diagonal line
plt.title("ROC–AUC Curve - LightGBM")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.grid(True)
plt.show()

model.fit(
    X_train_vec, y_train,
    eval_set=[(X_test_vec, y_test)],
    eval_metric=["logloss", "error"]   # error = 1 - accuracy
)

# Extract eval results
results = model.evals_result_

# LOSS CURVE (logloss)
plt.figure(figsize=(7, 5))
plt.plot(results["valid_0"]["binary_logloss"], label="Valid Logloss")
plt.title("LightGBM Loss Curve (Logloss)")
plt.xlabel("Iterations")
plt.ylabel("Logloss")
plt.grid(True)
plt.legend()
plt.show()

# ACCURACY CURVE (1 - error)
valid_error = results["valid_0"]["binary_error"]
valid_accuracy = [1 - x for x in valid_error]

plt.figure(figsize=(7, 5))
plt.plot(valid_accuracy, label="Valid Accuracy")
plt.title("LightGBM Accuracy Curve")
plt.xlabel("Iterations")
plt.ylabel("Accuracy")
plt.grid(True)
plt.legend()
plt.show()

